{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Variant Generation with S3 Storage\n",
    "\n",
    "This notebook:\n",
    "1. Queries a database for deal IDs\n",
    "2. Generates variant images using OpenAI\n",
    "3. Stores images in S3\n",
    "4. Implements async processing for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import time\n",
    "import base64\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import tempfile\n",
    "import sys\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS credentials\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Database for Deal IDs\n",
    "\n",
    "Execute SQL to get deal information including:\n",
    "- deal_voucher_id\n",
    "- original_image_id\n",
    "- variant_image_id\n",
    "- batch_name\n",
    "- enter_test_ts\n",
    "- exit_test_ts\n",
    "- open_ai_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/ipykernel_73706/1748472317.py:49: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>email_subject</th>\n",
       "      <th>category_name</th>\n",
       "      <th>sub_category_name</th>\n",
       "      <th>revenue_last_14_days</th>\n",
       "      <th>revenue_rank</th>\n",
       "      <th>image_id_pos_0</th>\n",
       "      <th>image_url_pos_0</th>\n",
       "      <th>extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33032041</td>\n",
       "      <td>4-Seater Rattan Garden Furniture Set</td>\n",
       "      <td>Garden</td>\n",
       "      <td>Garden Furniture</td>\n",
       "      <td>15989</td>\n",
       "      <td>1</td>\n",
       "      <td>1386187</td>\n",
       "      <td>https://static.wowcher.co.uk/images/deal/33032...</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32097419</td>\n",
       "      <td>Nine-Seater Garden Rattan Furniture Set</td>\n",
       "      <td>Garden</td>\n",
       "      <td>Garden Furniture</td>\n",
       "      <td>12384</td>\n",
       "      <td>2</td>\n",
       "      <td>1428878</td>\n",
       "      <td>https://static.wowcher.co.uk/images/deal/32097...</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39821204</td>\n",
       "      <td>Emma Hybrid Premium Mattress</td>\n",
       "      <td>Home</td>\n",
       "      <td>Beds &amp; Mattresses</td>\n",
       "      <td>11668</td>\n",
       "      <td>3</td>\n",
       "      <td>1581451</td>\n",
       "      <td>https://static.wowcher.co.uk/images/deal/39821...</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36958413</td>\n",
       "      <td>Rattan Furniture Set &amp; Fire Pit Table</td>\n",
       "      <td>Garden</td>\n",
       "      <td>Garden Furniture</td>\n",
       "      <td>6824</td>\n",
       "      <td>4</td>\n",
       "      <td>1447574</td>\n",
       "      <td>https://static.wowcher.co.uk/images/deal/36958...</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37259236</td>\n",
       "      <td>Divan Bed &amp; Memory Sprung Mattress</td>\n",
       "      <td>Home</td>\n",
       "      <td>Beds &amp; Mattresses</td>\n",
       "      <td>5314</td>\n",
       "      <td>5</td>\n",
       "      <td>1456986</td>\n",
       "      <td>https://static.wowcher.co.uk/images/deal/37259...</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                             email_subject category_name  \\\n",
       "0  33032041      4-Seater Rattan Garden Furniture Set        Garden   \n",
       "1  32097419  Nine-Seater Garden Rattan Furniture Set         Garden   \n",
       "2  39821204             Emma Hybrid Premium Mattress           Home   \n",
       "3  36958413     Rattan Furniture Set & Fire Pit Table        Garden   \n",
       "4  37259236        Divan Bed & Memory Sprung Mattress          Home   \n",
       "\n",
       "   sub_category_name  revenue_last_14_days  revenue_rank  image_id_pos_0  \\\n",
       "0   Garden Furniture                 15989             1         1386187   \n",
       "1   Garden Furniture                 12384             2         1428878   \n",
       "2  Beds & Mattresses                 11668             3         1581451   \n",
       "3   Garden Furniture                  6824             4         1447574   \n",
       "4  Beds & Mattresses                  5314             5         1456986   \n",
       "\n",
       "                                     image_url_pos_0 extension  \n",
       "0  https://static.wowcher.co.uk/images/deal/33032...       jpg  \n",
       "1  https://static.wowcher.co.uk/images/deal/32097...       jpg  \n",
       "2  https://static.wowcher.co.uk/images/deal/39821...       jpg  \n",
       "3  https://static.wowcher.co.uk/images/deal/36958...       jpg  \n",
       "4  https://static.wowcher.co.uk/images/deal/37259...       jpg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_deals_for_processing():\n",
    "    # Establish connection to Redshift\n",
    "    conn = psycopg2.connect(\n",
    "        host=os.environ.get(\"REDSHIFT_HOST\", \"bi-redshift.intwowcher.co.uk\"),\n",
    "        port=os.environ.get(\"REDSHIFT_PORT\", \"5439\"),\n",
    "        dbname=os.environ.get(\"REDSHIFT_DBNAME\", \"wowdwhprod\"),\n",
    "        user=os.environ.get(\"REDSHIFT_USER\", \"jenkins\"),\n",
    "        password=os.environ.get(\"REDSHIFT_PASSWORD\", \"9SDy1ffdfTV7\")\n",
    "    )\n",
    "    \n",
    "    # Example query - modify as needed\n",
    "    query = \"\"\"\n",
    "WITH revenue_per_deal AS (\n",
    "    SELECT deal_id, SUM(net) AS revenue_last_14_days\n",
    "    FROM real.transactions\n",
    "    WHERE order_date > sysdate - 14\n",
    "    GROUP BY deal_id\n",
    ")\n",
    "SELECT\n",
    "    CAST(dv.id AS INTEGER) AS id,\n",
    "    dv.email_subject,\n",
    "    dvc.name AS category_name,\n",
    "    dvsc.name AS sub_category_name,\n",
    "    CAST(COALESCE(rpd.revenue_last_14_days, 0) AS INTEGER) AS revenue_last_14_days,\n",
    "    CAST(rank() OVER (ORDER BY COALESCE(rpd.revenue_last_14_days, 0) DESC) AS INTEGER) AS revenue_rank,\n",
    "    CAST(dvi.id AS INTEGER) AS image_id_pos_0,\n",
    "    'https://static.wowcher.co.uk/images/deal/' || dvi.deal_voucher_id || '/' || dvi.id || '.' || dvi.extension AS image_url_pos_0,\n",
    "    dvi.extension\n",
    "FROM real.deal_voucher dv\n",
    "JOIN real.product p ON p.id = dv.id AND p.status_id = 1\n",
    "LEFT JOIN revenue_per_deal rpd ON rpd.deal_id = dv.id\n",
    "LEFT JOIN real.deal_voucher_site dvs ON dvs.deal_voucher_id = dv.id\n",
    "LEFT JOIN real.deal_voucher_image dvi ON dvi.deal_voucher_id = dv.id AND dvi.position = 0\n",
    "LEFT JOIN real.deal_voucher_category dvc ON dvc.id = dv.category_id\n",
    "LEFT JOIN real.deal_voucher_sub_category dvsc ON dvsc.id = dv.sub_category_id\n",
    "WHERE trunc(dv.closing_date) >= trunc(sysdate)\n",
    "    AND NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM temp.opt_image_variants oiv\n",
    "        WHERE oiv.deal_voucher_id = dv.id\n",
    "          AND (batch_name ILIKE '%manual%' AND status = 1)\n",
    "    )\n",
    "    AND dvc.canonical_path_type = 'NATIONAL'\n",
    "GROUP BY dv.id, dv.email_subject, dvc.name, dvsc.name, dvi.id, dvi.deal_voucher_id, dvi.extension,rpd.revenue_last_14_days\n",
    "ORDER BY COALESCE(rpd.revenue_last_14_days, 0) DESC\n",
    "LIMIT 500;\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return df\n",
    "# Get deals to process\n",
    "deals_df = get_deals_for_processing()\n",
    "deals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Upload Functions\n",
    "\n",
    "Functions to upload generated images to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(file_content, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Upload a file to S3\n",
    "    \n",
    "    Parameters:\n",
    "    - file_content: Binary content of the file\n",
    "    - bucket_name: S3 bucket name\n",
    "    - s3_key: Path in S3 where file will be stored\n",
    "    \n",
    "    Returns:\n",
    "    - URL of the uploaded file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine content type based on file extension\n",
    "        extension = os.path.splitext(s3_key)[1].lower()\n",
    "        content_type = 'image/jpeg' if extension in ['.jpg', '.jpeg'] else \\\n",
    "                      'image/png' if extension == '.png' else \\\n",
    "                      'image/webp' if extension == '.webp' else \\\n",
    "                      'application/octet-stream'\n",
    "                      \n",
    "        s3_client.put_object(\n",
    "            Body=file_content,\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            ContentType=content_type\n",
    "        )\n",
    "        return f\"https://static.wowcher.co.uk/{s3_key}\"\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "Function to call the generate_image.py script and process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized with API key: sk-pr...\n"
     ]
    }
   ],
   "source": [
    "# Cell to replace external script dependency with integrated functionality\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "\n",
    "# Load environment variables if not already done\n",
    "if 'client' not in locals():\n",
    "    load_dotenv()\n",
    "    client = OpenAI(api_key=os.getenv('OPEN_AI_API_KEY'))\n",
    "    print(f\"OpenAI client initialized with API key: {os.getenv('OPEN_AI_API_KEY')[:5]}...\")\n",
    "\n",
    "def get_deal_data_for_image(deal_id):\n",
    "    \"\"\"Get deal data needed for image generation\"\"\"\n",
    "    # Establish connection to Redshift\n",
    "    conn = psycopg2.connect(\n",
    "        host=os.environ.get(\"REDSHIFT_HOST\", \"bi-redshift.intwowcher.co.uk\"),\n",
    "        port=os.environ.get(\"REDSHIFT_PORT\", \"5439\"),\n",
    "        dbname=os.environ.get(\"REDSHIFT_DBNAME\", \"wowdwhprod\"),\n",
    "        user=os.environ.get(\"REDSHIFT_USER\", \"jenkins\"),\n",
    "        password=os.environ.get(\"REDSHIFT_PASSWORD\", \"9SDy1ffdfTV7\")\n",
    "    )\n",
    "\n",
    "    # Get email subject\n",
    "    email_subject_query = \"\"\"\n",
    "    SELECT email_subject \n",
    "    FROM wowdwhprod.real.deal_voucher\n",
    "    WHERE id = %s\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(email_subject_query, (deal_id,))\n",
    "        email_subject_result = cur.fetchone()\n",
    "        email_subject = email_subject_result[0] if email_subject_result else \"Deal\"\n",
    "\n",
    "    # Get image URLs and extract extension information\n",
    "    image_query = \"\"\"\n",
    "    SELECT \n",
    "        'https://static.wowcher.co.uk/images/deal/' || deal_voucher_id || '/' || id || '.' || extension AS image_url,\n",
    "        extension\n",
    "    FROM wowdwhprod.real.deal_voucher_image\n",
    "    WHERE deal_voucher_id = %s\n",
    "    ORDER BY position\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(image_query, (deal_id,))\n",
    "        image_results = cur.fetchall()\n",
    "        image_urls = [row[0] for row in image_results]\n",
    "        extensions = [row[1] for row in image_results]\n",
    "        original_extension = extensions[0] if extensions else \"png\"\n",
    "\n",
    "    # Get highlights\n",
    "    highlights_query = \"\"\"\n",
    "    SELECT highlight \n",
    "    FROM wowdwhprod.real.deal_voucher_highlight \n",
    "    WHERE deal_voucher_id = %s\n",
    "    limit 3\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(highlights_query, (deal_id,))\n",
    "        highlights_results = cur.fetchall()\n",
    "        highlights = [row[0] for row in highlights_results]\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Build the prompt\n",
    "    formatted_highlights = \"\\n\".join([f\"• {h}\" for h in highlights]) if highlights else \"\"\n",
    "    prompt = f\"\"\"\n",
    "Create ONE high-resolution hero image advertising **{email_subject}**.\n",
    "\n",
    "Final image must contain **zero spelling mistakes**.  \n",
    "\n",
    "1. **Source images** – You have multiple angles.  \n",
    "   • Accurately represent the product; do **not** invent new colours or features.  \n",
    "   • If variants exist, PICK ONE colour and keep it consistent.\n",
    "\n",
    "2. **Scene & background**  \n",
    "   • Place the product in a realistic, aspirational environment that makes sense for its use.  \n",
    "   • Adjust lighting and depth of field so the product is the clear focal point.  \n",
    "   • Background must not overpower or obscure the product.\n",
    "\n",
    "3. **Infographic & text elements**  \n",
    "    • Do **not** repeat the headline anywhere else in the artwork.  \n",
    "    • Any additional text must be limited to the 2-4 call-outs listed below.\n",
    "    \n",
    "   • Overlay 2-4 concise call-outs drawn from these highlights:  \n",
    "     {formatted_highlights}  \n",
    "   • Position all call-outs **outside** the bottom-right 20% of the frame.\n",
    "\n",
    "4. **Design constraints**  \n",
    "   • Keep bottom-right area completely free of any graphics or text.  \n",
    "   • Maintain 4 px padding around all text boxes.  \n",
    "   • No brand logos unless provided in the source images.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'image_urls': image_urls,\n",
    "        'original_extension': original_extension\n",
    "    }\n",
    "\n",
    "def download_image_to_file(url, filename):\n",
    "    \"\"\"Download an image from URL and save to file\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return filename\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image from {url}\")\n",
    "\n",
    "def generate_image_integrated(deal_id, original_id, temp_dir):\n",
    "    \"\"\"\n",
    "    Generate image using OpenAI's API (integrated version of generate_image.py)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing deal {deal_id} with original image {original_id}\")\n",
    "        # Get data for the deal\n",
    "        deal_data = get_deal_data_for_image(deal_id)\n",
    "        prompt = deal_data['prompt']\n",
    "        image_urls = deal_data['image_urls']\n",
    "        original_extension = deal_data['original_extension']\n",
    "        \n",
    "        # Create output filename\n",
    "        output_filename = os.path.join(temp_dir, f\"variant_{deal_id}_{original_id}.{original_extension}\")\n",
    "        \n",
    "        if not image_urls:\n",
    "            raise Exception(\"No images found for this deal\")\n",
    "            \n",
    "        print(f\"Found {len(image_urls)} images. Downloading up to 16 images...\")\n",
    "        \n",
    "        # Download images\n",
    "        image_files = []\n",
    "        temp_filenames = []\n",
    "        for idx, url in enumerate(image_urls[:16]):\n",
    "            temp_filename = os.path.join(temp_dir, f\"temp_image_{deal_id}_{idx}.png\")\n",
    "            download_image_to_file(url, temp_filename)\n",
    "            temp_filenames.append(temp_filename)\n",
    "            image_files.append(open(temp_filename, \"rb\"))\n",
    "            \n",
    "        print(\"Calling OpenAI API to edit images...\")\n",
    "        \n",
    "        # Call OpenAI API\n",
    "        result = client.images.edit(\n",
    "            model=\"gpt-image-1\",\n",
    "            image=image_files,\n",
    "            prompt=prompt,\n",
    "            size=\"1536x1024\",\n",
    "            quality=\"high\",\n",
    "            background=\"auto\",\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "        print(\"Received response from OpenAI API. Decoding and saving image...\")\n",
    "        \n",
    "        # Process and save the response\n",
    "        image_base64 = result.data[0].b64_json\n",
    "        image_bytes = base64.b64decode(image_base64)\n",
    "        with open(output_filename, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "            \n",
    "        # Close file handles\n",
    "        for f in image_files:\n",
    "            f.close()\n",
    "            \n",
    "        # Delete temporary files\n",
    "        for filename in temp_filenames:\n",
    "            if os.path.exists(filename):\n",
    "                try:\n",
    "                    os.remove(filename)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"Saved generated image to {output_filename}\")\n",
    "        \n",
    "        # Process token usage details\n",
    "        print(\"Token usage details:\")\n",
    "        token_info = {}\n",
    "        \n",
    "        total_tokens = result.usage.total_tokens\n",
    "        input_tokens = result.usage.input_tokens\n",
    "        output_tokens = result.usage.output_tokens\n",
    "        input_text_tokens = result.usage.input_tokens_details.text_tokens\n",
    "        input_image_tokens = result.usage.input_tokens_details.image_tokens\n",
    "        \n",
    "        token_info[\"Total tokens\"] = str(total_tokens)\n",
    "        token_info[\"Input tokens\"] = str(input_tokens)\n",
    "        token_info[\"Output tokens\"] = str(output_tokens)\n",
    "        token_info[\"Input text tokens\"] = str(input_text_tokens)\n",
    "        token_info[\"Input image tokens\"] = str(input_image_tokens)\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = (input_text_tokens * 5 + input_image_tokens * 10 + output_tokens * 40) / 1000000\n",
    "        token_info[\"Cost\"] = f\"${cost:.6f}\"\n",
    "        \n",
    "        print(f\"Cost: ${cost:.6f}\")\n",
    "        \n",
    "        return output_filename, original_extension, token_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image for deal {deal_id}: {str(e)}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Processing\n",
    "\n",
    "Process multiple deals in parallel using async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_deals_async(deals_df, max_workers=4, \n",
    "                              deal_id_col='deal_id',\n",
    "                              original_image_id_col='original_image_id'):\n",
    "    \"\"\"\n",
    "    Process multiple deals asynchronously using the integrated image generation function\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Display column names to help debugging\n",
    "    print(f\"Available columns in dataframe: {list(deals_df.columns)}\")\n",
    "    \n",
    "    # Create a temporary directory for image files\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Use ThreadPoolExecutor for parallelization\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Create tasks for all deals\n",
    "            futures = []\n",
    "            for idx, row in deals_df.iterrows():\n",
    "                # Get deal_id and original_id, with fallbacks if columns don't exist\n",
    "                deal_id = row.get(deal_id_col)\n",
    "                if deal_id is None and 'deal_voucher_id' in row:\n",
    "                    deal_id = row['deal_voucher_id']\n",
    "                elif deal_id is None and 'id' in row:\n",
    "                    deal_id = row['id']\n",
    "                \n",
    "                original_id = row.get(original_image_id_col)\n",
    "                if original_id is None and 'image_id' in row:\n",
    "                    original_id = row['image_id']\n",
    "                elif original_id is None:\n",
    "                    original_id = \"main\"  # Fallback if no image id is found\n",
    "                    \n",
    "                if deal_id is None:\n",
    "                    print(f\"Warning: Could not find deal ID in row: {row}\")\n",
    "                    continue\n",
    "                    \n",
    "                future = executor.submit(generate_image_integrated, deal_id, original_id, temp_dir)\n",
    "                futures.append((future, deal_id, original_id, row))\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future, deal_id, original_id, row in futures:\n",
    "                try:\n",
    "                    image_path, extension, token_info = future.result()\n",
    "                    \n",
    "                    if image_path:\n",
    "                        # Read the image file\n",
    "                        with open(image_path, 'rb') as img_file:\n",
    "                            img_content = img_file.read()\n",
    "                        \n",
    "                        # Upload to S3 with correct extension\n",
    "                        s3_key = f\"images/deal/{deal_id}/{original_id}_variant.{extension}\"\n",
    "                        s3_url = upload_to_s3(img_content, 'your-s3-bucket-name', s3_key)\n",
    "                        \n",
    "                        # Add to results\n",
    "                        result_row = row.to_dict()\n",
    "                        result_row.update({\n",
    "                            'status': 'success',\n",
    "                            's3_url': s3_url,\n",
    "                            'token_info': token_info,\n",
    "                            'extension': extension,\n",
    "                            'processed_timestamp': pd.Timestamp.now()\n",
    "                        })\n",
    "                        results.append(result_row)\n",
    "                    else:\n",
    "                        # Add failure to results\n",
    "                        result_row = row.to_dict()\n",
    "                        result_row.update({\n",
    "                            'status': 'failed',\n",
    "                            'error': 'Image generation failed',\n",
    "                            'processed_timestamp': pd.Timestamp.now()\n",
    "                        })\n",
    "                        results.append(result_row)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing deal {deal_id}: {str(e)}\")\n",
    "                    result_row = row.to_dict()\n",
    "                    result_row.update({\n",
    "                        'status': 'failed',\n",
    "                        'error': str(e),\n",
    "                        'processed_timestamp': pd.Timestamp.now()\n",
    "                    })\n",
    "                    results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Process\n",
    "\n",
    "Execute the async processing and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in dataframe: ['id', 'email_subject', 'category_name', 'sub_category_name', 'revenue_last_14_days', 'revenue_rank', 'image_id_pos_0', 'image_url_pos_0', 'extension']\n",
      "Processing deal 39832493 with original image main\n",
      "Processing deal 28773789 with original image main\n",
      "Processing deal 37564382 with original image main\n",
      "Processing deal 32223669 with original image main\n",
      "Processing deal 30021183 with original image main\n",
      "Found 10 images. Downloading up to 16 images...Found 2 images. Downloading up to 16 images...\n",
      "\n",
      "Found 6 images. Downloading up to 16 images...\n",
      "Found 8 images. Downloading up to 16 images...\n",
      "Found 5 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Calling OpenAI API to edit images...\n",
      "Calling OpenAI API to edit images...\n",
      "Calling OpenAI API to edit images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_37564382_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.257005\n",
      "Processing deal 40351043 with original image main\n",
      "Found 8 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_30021183_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.270255\n",
      "Processing deal 39551433 with original image main\n",
      "Found 2 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_39832493_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.276920\n",
      "Processing deal 31178068 with original image main\n",
      "Found 3 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Error processing deal 39832493: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_32223669_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.283840\n",
      "Processing deal 37811366 with original image main\n",
      "Found 6 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_28773789_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.266795\n",
      "Processing deal 39966290 with original image main\n",
      "Found 6 images. Downloading up to 16 images...\n",
      "Error processing deal 28773789: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Error processing deal 37564382: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not existCalling OpenAI API to edit images...\n",
      "\n",
      "Error processing deal 32223669: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Error processing deal 30021183: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_39551433_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.256820\n",
      "Processing deal 32488332 with original image main\n",
      "Found 3 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_31178068_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.260090\n",
      "Processing deal 39790931 with original image main\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_40351043_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.277040\n",
      "Processing deal 38845870 with original image main\n",
      "Found 10 images. Downloading up to 16 images...\n",
      "Found 8 images. Downloading up to 16 images...\n",
      "Error processing deal 40351043: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Calling OpenAI API to edit images...\n",
      "Error processing deal 39551433: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Calling OpenAI API to edit images...\n",
      "Error processing deal 31178068: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_37811366_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.270290\n",
      "Processing deal 39382316 with original image main\n",
      "Found 10 images. Downloading up to 16 images...\n",
      "Error processing deal 37811366: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_39966290_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.270150\n",
      "Processing deal 33931188 with original image main\n",
      "Found 1 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Error processing deal 39966290: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_32488332_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.260105\n",
      "Processing deal 22785446 with original image main\n",
      "Error processing deal 32488332: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Found 6 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_39790931_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.284075\n",
      "Processing deal 22772737 with original image main\n",
      "Found 6 images. Downloading up to 16 images...\n",
      "Error processing deal 39790931: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Calling OpenAI API to edit images...\n",
      "Error generating image for deal 38845870: Error code: 400 - {'error': {'message': 'Your request was rejected as a result of our safety system. Your request may contain content that is not allowed by our safety system.', 'type': 'user_error', 'param': None, 'code': 'moderation_blocked'}}\n",
      "Processing deal 32489162 with original image main\n",
      "Found 3 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_39382316_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.283755\n",
      "Processing deal 32583238 with original image main\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_33931188_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.253675\n",
      "Processing deal 39311258 with original image main\n",
      "Found 3 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Error processing deal 39382316: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Found 10 images. Downloading up to 16 images...\n",
      "Error processing deal 33931188: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_32489162_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.260160\n",
      "Processing deal 26944297 with original image main\n",
      "Found 5 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_22785446_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.270125\n",
      "Processing deal 22046640 with original image main\n",
      "Error processing deal 22785446: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Found 1 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_22772737_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.270140\n",
      "Processing deal 38948031 with original image main\n",
      "Found 5 images. Downloading up to 16 images...\n",
      "Error processing deal 22772737: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Calling OpenAI API to edit images...\n",
      "Error processing deal 32489162: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_32583238_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.260235\n",
      "Processing deal 35178642 with original image main\n",
      "Found 10 images. Downloading up to 16 images...\n",
      "Error processing deal 32583238: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_39311258_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.283390\n",
      "Processing deal 29304907 with original image main\n",
      "Error processing deal 39311258: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Found 4 images. Downloading up to 16 images...\n",
      "Calling OpenAI API to edit images...\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_26944297_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.266955\n",
      "Error processing deal 26944297: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_38948031_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.266785\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_22046640_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.253370\n",
      "Error processing deal 22046640: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Error processing deal 38948031: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_35178642_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.283855\n",
      "Error processing deal 35178642: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n",
      "Received response from OpenAI API. Decoding and saving image...\n",
      "Saved generated image to /var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/tmpgst7wwl0/variant_29304907_main.jpg\n",
      "Token usage details:\n",
      "Cost: $0.263485\n",
      "Error processing deal 29304907: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['deal_voucher_id', 'original_image_id', 's3_url'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_deals_async(deals_df_tail, max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m results_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeal_voucher_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_image_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextension\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['deal_voucher_id', 'original_image_id', 's3_url'] not in index\""
     ]
    }
   ],
   "source": [
    "deals_df_tail = deals_df.tail(25)\n",
    "\n",
    "# Process deals asynchronously\n",
    "results_df = await process_deals_async(deals_df_tail, max_workers=5)\n",
    "\n",
    "# Display results\n",
    "results_df[['deal_voucher_id', 'original_image_id', 'status', 's3_url', 'extension', 'processed_timestamp']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Calculate statistics and costs across the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize processing results\n",
    "success_count = results_df[results_df['status'] == 'success'].shape[0]\n",
    "failed_count = results_df[results_df['status'] == 'failed'].shape[0]\n",
    "total_count = len(results_df)\n",
    "\n",
    "print(f\"Processing Summary:\")\n",
    "print(f\"Total deals: {total_count}\")\n",
    "print(f\"Successful: {success_count} ({success_count/total_count*100:.1f}%)\")\n",
    "print(f\"Failed: {failed_count} ({failed_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Calculate total cost \n",
    "total_cost = 0.0\n",
    "for idx, row in results_df.iterrows():\n",
    "    if row['status'] == 'success' and 'token_info' in row and row['token_info'] and 'Cost' in row['token_info']:\n",
    "        cost_str = row['token_info']['Cost'].replace('$', '')\n",
    "        try:\n",
    "            cost = float(cost_str)\n",
    "            total_cost += cost\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"\\nTotal estimated cost for this batch: ${total_cost:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
