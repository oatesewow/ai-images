{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Variant Generation with S3 Storage\n",
    "\n",
    "This notebook:\n",
    "1. Queries a database for deal IDs\n",
    "2. Generates variant images using OpenAI\n",
    "3. Stores images in S3\n",
    "4. Implements async processing for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import time\n",
    "import base64\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import tempfile\n",
    "import sys\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure AWS credentials\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Database for Deal IDs\n",
    "\n",
    "Execute SQL to get deal information including:\n",
    "- deal_voucher_id\n",
    "- original_image_id\n",
    "- variant_image_id\n",
    "- batch_name\n",
    "- enter_test_ts\n",
    "- exit_test_ts\n",
    "- open_ai_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/zpttw0wn0192zryb4bdz19lr0000gn/T/ipykernel_24186/257375721.py:65: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "def get_deals_for_processing():\n",
    "    # Establish connection to Redshift\n",
    "    conn = psycopg2.connect(\n",
    "        host=os.environ.get(\"REDSHIFT_HOST\", \"bi-redshift.intwowcher.co.uk\"),\n",
    "        port=os.environ.get(\"REDSHIFT_PORT\", \"5439\"),\n",
    "        dbname=os.environ.get(\"REDSHIFT_DBNAME\", \"wowdwhprod\"),\n",
    "        user=os.environ.get(\"REDSHIFT_USER\", \"jenkins\"),\n",
    "        password=os.environ.get(\"REDSHIFT_PASSWORD\", \"9SDy1ffdfTV7\")\n",
    "    )\n",
    "    \n",
    "    # Example query - modify as needed\n",
    "    query = \"\"\"\n",
    "WITH visitors AS (\n",
    "    SELECT\n",
    "        deal_id_evar,\n",
    "        COUNT(DISTINCT visitor_id) AS visitors\n",
    "    FROM real.omniture_events\n",
    "    WHERE trunc(date_time) >= trunc(sysdate) - 7\n",
    "      AND product = 'wowdtm'\n",
    "      AND (\n",
    "            url_evar LIKE '%/deal/%' OR\n",
    "            url_evar LIKE '%/e/%' OR\n",
    "            url_evar LIKE '%/email-deals/%'\n",
    "          )\n",
    "    GROUP BY deal_id_evar\n",
    ")\n",
    "SELECT\n",
    "    CAST(dv.id AS INTEGER) AS id,\n",
    "    dv.email_subject,\n",
    "    dvc.name AS category_name,\n",
    "    dvsc.name AS sub_category_name,\n",
    "    CAST(COALESCE(v.visitors, 0) AS INTEGER) AS visitors_last_7_days,\n",
    "    CAST(rank() OVER (ORDER BY COALESCE(v.visitors, 0) DESC) AS INTEGER) AS visitor_rank,\n",
    "    CAST(dvi.id AS INTEGER) AS image_id_pos_0,\n",
    "    'https://static.wowcher.co.uk/images/deal/' || dvi.deal_voucher_id || '/' || dvi.id || '.' || dvi.extension AS image_url_pos_0,\n",
    "    dvi.extension\n",
    "FROM real.deal_voucher dv\n",
    "JOIN real.product p ON p.id = dv.id AND p.status_id = 1\n",
    "LEFT JOIN visitors v ON v.deal_id_evar = dv.id\n",
    "LEFT JOIN real.deal_voucher_site dvs ON dvs.deal_voucher_id = dv.id\n",
    "LEFT JOIN real.deal_voucher_image dvi ON dvi.deal_voucher_id = dv.id AND dvi.position = 0\n",
    "LEFT JOIN real.deal_voucher_category dvc ON dvc.id = dv.category_id\n",
    "LEFT JOIN real.deal_voucher_sub_category dvsc ON dvsc.id = dv.sub_category_id\n",
    "LEFT JOIN real.site s ON s.id = dv.deal_location_id AND s.site_name = 'National Deal'\n",
    "WHERE trunc(dv.closing_date) >= trunc(sysdate)\n",
    "AND dv.currency = 'GBP'\n",
    "AND NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM temp.opt_image_variants oiv\n",
    "    WHERE oiv.deal_voucher_id = dv.id\n",
    "    AND (\n",
    "        ((batch_name ILIKE '%manual%' AND status IN (1,3,5))\n",
    "        OR (batch_name = 'OPEN AI Images' AND status IN (1,3))\n",
    "        OR (batch_name NOT IN ('Manual Opt', 'OPEN AI Images') AND status = 1)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "and dvc.name = 'Garden'\n",
    "AND dvc.canonical_path_type = 'NATIONAL'\n",
    "GROUP BY dv.id, dv.email_subject, dvc.name, dvsc.name, dvi.id, dvi.deal_voucher_id, dvi.extension, v.visitors\n",
    "ORDER BY COALESCE(v.visitors, 0) DESC\n",
    "LIMIT 2000;\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return df\n",
    "# Get deals to process\n",
    "deals_df = get_deals_for_processing()\n",
    "deals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Upload Functions\n",
    "\n",
    "Functions to upload generated images to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into 10 parts\n",
    "df_splits = [deals_df.iloc[i:i + 1000] for i in range(0, len(deals_df), 1000)]\n",
    "# Assign each split to a separate variable\n",
    "df_1000, df_2000, df_3000, df_4000, df_5000 = df_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(file_content, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Upload a file to S3\n",
    "    \n",
    "    Parameters:\n",
    "    - file_content: Binary content of the file\n",
    "    - bucket_name: S3 bucket name\n",
    "    - s3_key: Path in S3 where file will be stored\n",
    "    \n",
    "    Returns:\n",
    "    - URL of the uploaded file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine content type based on file extension\n",
    "        extension = os.path.splitext(s3_key)[1].lower()\n",
    "        content_type = 'image/jpeg' if extension in ['.jpg', '.jpeg'] else \\\n",
    "                      'image/png' if extension == '.png' else \\\n",
    "                      'image/webp' if extension == '.webp' else \\\n",
    "                      'application/octet-stream'\n",
    "                      \n",
    "        s3_client.put_object(\n",
    "            Body=file_content,\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            ContentType=content_type\n",
    "        )\n",
    "        return f\"https://static.wowcher.co.uk/{s3_key}\"\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "Function to call the generate_image.py script and process the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to replace external script dependency with integrated functionality\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "\n",
    "# Load environment variables if not already done\n",
    "if 'client' not in locals():\n",
    "    load_dotenv()\n",
    "    client = OpenAI(api_key=os.getenv('OPEN_AI_API_KEY'))\n",
    "    print(f\"OpenAI client initialized with API key: {os.getenv('OPEN_AI_API_KEY')[:5]}...\")\n",
    "\n",
    "def get_deal_data_for_image(deal_id):\n",
    "    \"\"\"Get deal data needed for image generation\"\"\"\n",
    "    # Establish connection to Redshift\n",
    "    conn = psycopg2.connect(\n",
    "        host=os.environ.get(\"REDSHIFT_HOST\", \"bi-redshift.intwowcher.co.uk\"),\n",
    "        port=os.environ.get(\"REDSHIFT_PORT\", \"5439\"),\n",
    "        dbname=os.environ.get(\"REDSHIFT_DBNAME\", \"wowdwhprod\"),\n",
    "        user=os.environ.get(\"REDSHIFT_USER\", \"jenkins\"),\n",
    "        password=os.environ.get(\"REDSHIFT_PASSWORD\", \"9SDy1ffdfTV7\")\n",
    "    )\n",
    "\n",
    "    # Get email subject\n",
    "    email_subject_query = \"\"\"\n",
    "    SELECT email_subject \n",
    "    FROM wowdwhprod.real.deal_voucher\n",
    "    WHERE id = %s\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(email_subject_query, (deal_id,))\n",
    "        email_subject_result = cur.fetchone()\n",
    "        email_subject = email_subject_result[0] if email_subject_result else \"Deal\"\n",
    "\n",
    "    # Get image URLs and extract extension information\n",
    "    image_query = \"\"\"\n",
    "    SELECT \n",
    "        'https://static.wowcher.co.uk/images/deal/' || deal_voucher_id || '/' || id || '.' || extension AS image_url,\n",
    "        extension\n",
    "    FROM wowdwhprod.real.deal_voucher_image\n",
    "    WHERE deal_voucher_id = %s\n",
    "    ORDER BY position\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(image_query, (deal_id,))\n",
    "        image_results = cur.fetchall()\n",
    "        image_urls = [row[0] for row in image_results]\n",
    "        extensions = [row[1] for row in image_results]\n",
    "        original_extension = extensions[0] if extensions else \"png\"\n",
    "\n",
    "    # Get highlights\n",
    "    highlights_query = \"\"\"\n",
    "    SELECT\n",
    "    SPLIT_PART(highlight, ':', 1) AS highlight\n",
    "    FROM wowdwhprod.real.deal_voucher_highlight\n",
    "    WHERE deal_voucher_id = %s\n",
    "    LIMIT 3;\n",
    "\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(highlights_query, (deal_id,))\n",
    "        highlights_results = cur.fetchall()\n",
    "        highlights = [row[0] for row in highlights_results]\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Build the prompt\n",
    "    formatted_highlights = \"\\n\".join([f\"• {h}\" for h in highlights]) if highlights else \"\"\n",
    "    prompt = f\"\"\"\n",
    "Create ONE high-resolution hero image advertising **{email_subject}**.\n",
    "\n",
    "Final image must contain **zero spelling mistakes**.  \n",
    "\n",
    "1. **Source images** – You have multiple angles.  \n",
    "   • Accurately represent the product; do **not** invent new colours or features.  \n",
    "   • If colour variants exist, PICK ONE colour and keep it consistent, though you should highlight the colours available in the add or the fact that there are multiple colours.\n",
    "   - do not put any prices in the image. \n",
    "\n",
    "2. **Scene & background**  \n",
    "   • Place the product in a realistic, aspirational environment that makes sense for its use.  \n",
    "   • Adjust lighting and depth of field so the product is the clear focal point.  \n",
    "   • Background must not overpower or obscure the product.\n",
    "\n",
    "3. **Infographic & text elements**  \n",
    "    • Do **not** repeat the headline anywhere else in the artwork.  \n",
    "    • Any additional text or graphics must be limited to the 2-4 call-outs listed below.\n",
    "    - Try and place the additional text or stickers on the left of the image. \n",
    "   • Overlay 2-4 concise call-outs drawn from these highlights:  \n",
    "     {formatted_highlights}  \n",
    "   • Position all call-outs **outside** the bottom-right 20% of the frame.\n",
    "\n",
    "4. **Design constraints**  \n",
    "   • Keep bottom-right area completely free of any graphics or text. \n",
    "   • Maintain 4 px padding around all text boxes.  \n",
    "   • No brand logos unless provided in the source images.\n",
    "\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'image_urls': image_urls,\n",
    "        'original_extension': original_extension\n",
    "    }\n",
    "\n",
    "def download_image_to_file(url, filename):\n",
    "    \"\"\"Download an image from URL and save to file\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return filename\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image from {url}\")\n",
    "# Add this to your imports\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import logging\n",
    "\n",
    "# Set up logging to control verbosity\n",
    "logging.basicConfig(level=logging.WARNING)  # Set to WARNING to hide INFO and DEBUG messages\n",
    "\n",
    "def generate_image_integrated(deal_id, original_id, temp_dir, verbose=False):\n",
    "    \"\"\"\n",
    "    Generate image using OpenAI's API with minimal output\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if verbose:\n",
    "            print(f\"Processing deal {deal_id}\")\n",
    "        \n",
    "        # Get data for the deal\n",
    "        deal_data = get_deal_data_for_image(deal_id)\n",
    "        prompt = deal_data['prompt']\n",
    "        image_urls = deal_data['image_urls']\n",
    "        original_extension = deal_data['original_extension']\n",
    "        \n",
    "        # Create output filename\n",
    "        output_filename = os.path.join(temp_dir, f\"variant_{deal_id}_{original_id}.{original_extension}\")\n",
    "        \n",
    "        if not image_urls:\n",
    "            raise Exception(\"No images found for this deal\")\n",
    "        \n",
    "        # Download images silently\n",
    "        image_files = []\n",
    "        temp_filenames = []\n",
    "        for idx, url in enumerate(image_urls[:16]):\n",
    "            temp_filename = os.path.join(temp_dir, f\"temp_image_{deal_id}_{idx}.png\")\n",
    "            download_image_to_file(url, temp_filename)\n",
    "            temp_filenames.append(temp_filename)\n",
    "            image_files.append(open(temp_filename, \"rb\"))\n",
    "        \n",
    "        # Call OpenAI API\n",
    "        result = client.images.edit(\n",
    "            model=\"gpt-image-1\",\n",
    "            image=image_files,\n",
    "            prompt=prompt,\n",
    "            size=\"1536x1024\",\n",
    "            quality=\"high\",\n",
    "            background=\"auto\",\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "        # Process and save the response\n",
    "        image_base64 = result.data[0].b64_json\n",
    "        image_bytes = base64.b64decode(image_base64)\n",
    "        with open(output_filename, \"wb\") as f:\n",
    "            f.write(image_bytes)\n",
    "            \n",
    "        # Close file handles\n",
    "        for f in image_files:\n",
    "            f.close()\n",
    "            \n",
    "        # Delete temporary files\n",
    "        for filename in temp_filenames:\n",
    "            if os.path.exists(filename):\n",
    "                try:\n",
    "                    os.remove(filename)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Process token usage details silently\n",
    "        token_info = {}\n",
    "        if hasattr(result, 'usage'):\n",
    "            total_tokens = result.usage.total_tokens\n",
    "            input_tokens = result.usage.input_tokens\n",
    "            output_tokens = result.usage.output_tokens\n",
    "            input_text_tokens = result.usage.input_tokens_details.text_tokens\n",
    "            input_image_tokens = result.usage.input_tokens_details.image_tokens\n",
    "            \n",
    "            token_info[\"Total tokens\"] = str(total_tokens)\n",
    "            token_info[\"Input tokens\"] = str(input_tokens)\n",
    "            token_info[\"Output tokens\"] = str(output_tokens)\n",
    "            token_info[\"Input text tokens\"] = str(input_text_tokens)\n",
    "            token_info[\"Input image tokens\"] = str(input_image_tokens)\n",
    "            \n",
    "            # Calculate cost\n",
    "            cost = (input_text_tokens * 5 + input_image_tokens * 10 + output_tokens * 40) / 1000000\n",
    "            token_info[\"Cost\"] = f\"${cost:.6f}\"\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Cost: ${cost:.6f}\")\n",
    "        \n",
    "        return output_filename, original_extension, token_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error generating image for deal {deal_id}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "async def process_deals_async(deals_df, max_workers=4, verbose=False):\n",
    "    \"\"\"\n",
    "    Process multiple deals asynchronously with clean, minimal output\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Available columns in dataframe: {list(deals_df.columns)}\")\n",
    "    \n",
    "    # Create a temporary directory for image files\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Use ThreadPoolExecutor for parallelization\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Create tasks for all deals\n",
    "            futures = []\n",
    "            for idx, row in deals_df.iterrows():\n",
    "                # Get deal_id and original_id from the actual column names\n",
    "                deal_id = row.get('id')\n",
    "                original_id = row.get('image_id_pos_0', 'main')\n",
    "                    \n",
    "                if deal_id is None:\n",
    "                    if verbose:\n",
    "                        print(f\"Warning: Could not find deal ID in row: {row}\")\n",
    "                    continue\n",
    "                    \n",
    "                future = executor.submit(generate_image_integrated, deal_id, original_id, temp_dir, verbose)\n",
    "                futures.append((future, deal_id, original_id, row))\n",
    "            \n",
    "            # Create just ONE overall progress bar\n",
    "            print(f\"Processing {len(futures)} deals...\")\n",
    "            progress_bar = tqdm(total=len(futures), desc=\"Overall progress\")\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future, deal_id, original_id, row in futures:\n",
    "                try:\n",
    "                    image_path, extension, token_info = future.result()\n",
    "                    \n",
    "                    if image_path:\n",
    "                        # Read the image file\n",
    "                        with open(image_path, 'rb') as img_file:\n",
    "                            img_content = img_file.read()\n",
    "                        \n",
    "                        # Upload to S3 with correct extension\n",
    "                        s3_key = f\"images/deal/{deal_id}/{original_id}_variant.{extension}\"\n",
    "                        s3_url = upload_to_s3(img_content, 'static.wowcher.co.uk', s3_key)\n",
    "                        \n",
    "                        # Add to results\n",
    "                        result_row = row.to_dict()\n",
    "                        result_row.update({\n",
    "                            'status': 'success',\n",
    "                            's3_url': s3_url,\n",
    "                            'token_info': token_info,\n",
    "                            'extension': extension,\n",
    "                            'processed_timestamp': pd.Timestamp.now()\n",
    "                        })\n",
    "                        results.append(result_row)\n",
    "                    else:\n",
    "                        # Add failure to results\n",
    "                        result_row = row.to_dict()\n",
    "                        result_row.update({\n",
    "                            'status': 'failed',\n",
    "                            'error': 'Image generation failed',\n",
    "                            'processed_timestamp': pd.Timestamp.now()\n",
    "                        })\n",
    "                        results.append(result_row)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Error processing deal {deal_id}: {str(e)}\")\n",
    "                    result_row = row.to_dict()\n",
    "                    result_row.update({\n",
    "                        'status': 'failed',\n",
    "                        'error': str(e),\n",
    "                        'processed_timestamp': pd.Timestamp.now()\n",
    "                    })\n",
    "                    results.append(result_row)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.update(1)\n",
    "            \n",
    "            # Close progress bar\n",
    "            progress_bar.close()\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Process\n",
    "\n",
    "Execute the async processing and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deals_sample = deals_df\n",
    "\n",
    "print(f\"Selected {len(deals_sample)} deals\")\n",
    "\n",
    "# Process deals asynchronously\n",
    "results_df = await process_deals_async(deals_sample, max_workers=50, verbose=False)\n",
    "\n",
    "# Filter to get only successful results (the \"winners\")\n",
    "winners_df = results_df[results_df['status'] == 'success'].copy()\n",
    "print(f\"\\nSuccessful generations: {len(winners_df)} out of {len(results_df)}\")\n",
    "\n",
    "# Display the winners\n",
    "display(winners_df)\n",
    "\n",
    "# Calculate total cost for successful generations\n",
    "if 'token_info' in winners_df.columns:\n",
    "    total_cost = 0.0\n",
    "    for _, row in winners_df.iterrows():\n",
    "        if 'token_info' in row and row['token_info'] and 'Cost' in row['token_info']:\n",
    "            cost_str = row['token_info']['Cost'].replace('$', '')\n",
    "            try:\n",
    "                total_cost += float(cost_str)\n",
    "            except:\n",
    "                pass\n",
    "    print(f\"Total cost for successful generations: ${total_cost:.4f}\")\n",
    "    \n",
    "winners_df.to_csv('4000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving winners into test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('All500Approved.csv', index_col=0)\n",
    "df = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration variables\n",
    "BATCH_NAME = \"AI_Replicate_rattan\"\n",
    "MAX_WORKERS = 10\n",
    "CSV_OUTPUT_FILE = 'processed_images_with_variants.csv'\n",
    "DB_RECORDS_FILE = 'db_records.csv'\n",
    "REDSHIFT_TABLE = 'temp.opt_image_variants'\n",
    "\n",
    "# Assuming results_df is your dataframe with the processed images\n",
    "# If you need to load it from CSV instead:\n",
    "# results_df = pd.read_csv('Rattan_FurnitureALL.csv')\n",
    "\n",
    "def process_single_image(args):\n",
    "    index, row, s3_client, bucket_name = args\n",
    "    try:\n",
    "        # Get the generated image URL\n",
    "        generated_url = row['generated_url']\n",
    "        deal_id = str(row['deal_id'])\n",
    "        image_id = str(row['image_id'])\n",
    "        \n",
    "        # Skip if no generated URL\n",
    "        if pd.isna(generated_url):\n",
    "            print(f\"Skipping row {index}: No generated URL\")\n",
    "            return index, None, None\n",
    "            \n",
    "        # Download the generated image\n",
    "        response = requests.get(generated_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to download image for deal {deal_id}\")\n",
    "            return index, None, None\n",
    "            \n",
    "        # Create variant image ID (original ID * 100000)\n",
    "        variant_image_id = int(image_id) * 100000\n",
    "        \n",
    "        # Create new key with variant ID\n",
    "        new_key = f\"images/deal/{deal_id}/{variant_image_id}.jpg\"\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=new_key,\n",
    "            Body=response.content,\n",
    "            ContentType='image/jpeg',\n",
    "            CacheControl='no-cache'\n",
    "        )\n",
    "        \n",
    "        # Return the final URL\n",
    "        final_url = f\"https://{bucket_name}/{new_key}\"\n",
    "        return index, final_url, variant_image_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing deal {deal_id}: {str(e)}\")\n",
    "        return index, None, None\n",
    "\n",
    "def copy_generated_images(df):\n",
    "    # Connect to S3\n",
    "    s3_client = boto3.client('s3', **AWS_CONFIG)\n",
    "    bucket_name = S3_CONFIG['bucket_name']\n",
    "    \n",
    "    # Add new columns for final URL and variant image ID\n",
    "    df['final_url'] = None\n",
    "    df['variant_image_id'] = None\n",
    "\n",
    "    # Create arguments for each row\n",
    "    args_list = [(idx, row, s3_client, bucket_name) \n",
    "                 for idx, row in df.iterrows()]\n",
    "\n",
    "    # Process images concurrently using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(process_single_image, args) \n",
    "                  for args in args_list]\n",
    "        \n",
    "        with tqdm(total=len(df), desc=\"Copying Images\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                idx, final_url, variant_id = future.result()\n",
    "                if final_url:\n",
    "                    df.loc[idx, 'final_url'] = final_url\n",
    "                    df.loc[idx, 'variant_image_id'] = variant_id\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(\"Processing complete\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def copy_s3_to_redshift(s3_url):\n",
    "    \"\"\"Copy data from S3 to Redshift\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(**REDSHIFT_CONFIG)\n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        s3_path = s3_url.replace(f'https://{S3_CONFIG[\"bucket_name\"]}/', f's3://{S3_CONFIG[\"bucket_name\"]}/')\n",
    "        \n",
    "        # Added explicit column mapping\n",
    "        copy_command = f\"\"\"\n",
    "        COPY {REDSHIFT_TABLE}(\n",
    "            deal_voucher_id,\n",
    "            claid_prompt,\n",
    "            status,\n",
    "            original_image_id,\n",
    "            variant_image_id,\n",
    "            batch_name, \n",
    "            enter_test_ts\n",
    "        )\n",
    "        FROM '{s3_path}'\n",
    "        ACCESS_KEY_ID '{AWS_CONFIG[\"aws_access_key_id\"]}'\n",
    "        SECRET_ACCESS_KEY '{AWS_CONFIG[\"aws_secret_access_key\"]}'\n",
    "        CSV\n",
    "        IGNOREHEADER 1\n",
    "        ACCEPTINVCHARS AS '^'\n",
    "        MAXERROR 10;\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(copy_command)\n",
    "        connection.commit()\n",
    "        \n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {REDSHIFT_TABLE} WHERE batch_name = '{BATCH_NAME}'\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"Successfully copied {row_count} rows to Redshift table\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error copying to Redshift: {str(e)}\")\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor:\n",
    "            cursor.close()\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.close()\n",
    "\n",
    "# Use the function with the S3 URL from previous upload\n",
    "if s3_url:\n",
    "    copy_s3_to_redshift(s3_url)\n",
    "    \n",
    "def update_test_list():\n",
    "    \"\"\"Update the test list in the API after adding new variants\"\"\"\n",
    "    try:\n",
    "        # Initialize connection\n",
    "        connection = psycopg2.connect(**REDSHIFT_CONFIG)\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Get all active image IDs\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT original_image_id \n",
    "            FROM temp.opt_image_variants\n",
    "            WHERE status = 1\n",
    "            GROUP BY original_image_id\n",
    "        \"\"\")\n",
    "        \n",
    "        # Format image IDs for API\n",
    "        image_ids = [f\":{str(row[0])}\" for row in cursor.fetchall()]\n",
    "        \n",
    "        # Set up API headers\n",
    "        headers = {\n",
    "            \"x-wowsecret\": API_CONFIG['secret'],\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            'Cookie': 'coreCookie=react; dp=c; landing_page=a; landing_page2=a'\n",
    "        }\n",
    "        \n",
    "        # Update the API with the list\n",
    "        response = requests.post(\n",
    "            \"https://www.wowcher.co.uk/deal-variant-db/deal/set?dv_id=imgv_list_wow_uk\",\n",
    "            headers=headers,\n",
    "            json=image_ids\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        print(f\"Successfully updated test list with {len(image_ids)} images\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating test list: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if 'cursor' in locals() and cursor:\n",
    "            cursor.close()\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.close()\n",
    "\n",
    "# Use the function with the S3 URL from previous upload\n",
    "if s3_url:\n",
    "    print(\"Updating test list with new variants...\")\n",
    "    update_test_list()\n",
    "else:\n",
    "    print(\"Skipping test list update due to database error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
